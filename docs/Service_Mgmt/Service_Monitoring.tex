\subsubsection{Scope}
This section describes operational concepts of systems-level and service-level
monitoring for services operated by the LSST Data Facility.

\subsubsection{Overview}

\paragraph{Description}

The service monitoring system is the source of truth for the health and status
of all operational services within its scope. The monitoring system deals with
quality controls related to service delivery. These data have both retrospective
and real-time uses:

\begin{itemize}

\item Acquires data from subordinate monitoring systems within components that
are not bespoken LSST software. These monitoring systems may have an API, log
files, SNMP, and TBD other interfaces.

\item Acquires data from native LSST interfaces, including interfacing to the
logging package (lsst.log),  Prompt logging, Prompt events,
scoreboards (Redis), TBD Qserv, and data from other independent packages.

\item Probes services from monitoring agents and ingests quality control
parameters.

\item Synthesizes new quality control data from existing quality control data
(for example, correlating a series of events before generating an event that
will issue a page).

\item Can generate events based on performance or malfunction which can trigger
incident response for services and ITC, including to a non-NCSA incident
response software.

\item Can generate reports used for problem management, availability management,
capacity management, vendor management and similar processes.

\item Provides dashboard (or comfort) displays satisfying the use cases defined
below.

\item Provides for instantiation of displays anywhere within the LSST
operational environment (concerns porting vs. remote display, paint display with
high latency).

\item Provides for publicly visible displays and displays visible only to those
authorized by the LSST Authentication and Authorization system.

\item Is sensitive to dynamic deployment of services to ITC resources.

\item Is sensitive to modes of deployment and test, integration, development
when generating alerts, painting displays, and recording data for retrospective
use (concerns segregation and separation).

\item Is itself highly reliable and available.

\item Provides for disconnected operations between geographic sites (Summit,
Base and NCSA) and enclaves (e.g., Observing Critical and non-Observing
Critical).

\end{itemize}

\paragraph{Objective}

The set of services and infrastructure relied on by LSST Data Facility
operations is inherently distributed due to the distributed deployment of the
LDF services. Reliable operations of LDF services involves components
instantiated (at least) in Chile, at NCSA, and at CC-IN2P3, as well as the
networks between these sites.

A dataset based on the operational characteristics of the facilities, hardware,
software and other elements of service infrastructure is needed to support
service management, service delivery, service transition, and ITC-level
activities, as well as to provide health and status information to the users of
the systems. This dataset must be substantially unified, so that all activities
are supported by a single source of truth. From a unified dataset, for example,
staff concerned with availability management of a service can obtain records
that consistently reflect availability information generated by incident response
activities, while staff concerned with capacity management can obtain
information on how capacity is provided by ITC activities.

In general, service management needs both a subset of the data that is needed
for ITC management and data which may not be supplied by traditional ITC
monitoring. Examples of data not supplied by ITC monitoring include the
end-to-end availability of a service that tolerates hardware faults, user-facing
comfort displays which address specific areas of interest, and controls that
monitor data flow into disaster recovery stores for consistency with creation of data.

\paragraph{Operational Context}

LDF services rely on ITC hosted at NCSA, the Chilean Base Center, satellite
computing centers, test stands at LSST Headquarters, wide area networks, and
possibly other sources of infrastructure. Each of these sources possesses
organization-specific (non-uniform) ITC monitoring and service management
information on which LDF services rely. In all cases the LSST Data Facility needs
to centrally acquire sufficient data to provide for management of LDF services,
while minimizing coupling to the ITC or service provisioning from these sites.
The coupling should be defined in an internal Service Level Agreement (SLA) or
similar written instrument.

\subsubsection{Operational Concepts}

\paragraph{Normal Operations}

\subparagraph{Example: Prompt Products Services}

Prompt Products services are instantiated at NCSA and the Chilean Base Center. The
services the LDF relies on that may provide monitoring information are described
in the table below. The monitoring system may acquire additional essential data
by agents, consistent with SLAs and systems engineering best practice.

\begin{longtable}{|p{0.5\textwidth}|p{0.5\textwidth}|}
\caption{Sources of Monitoring Data} \\\hline
\textbf{Reliance} & \textbf{Subordinate monitoring interfaces provided} \\\hline
WAN from Chilean border router to UIUC & Wide area network activity area of LDF \\\hline
Network transit from Chilean prompt processing  infrastructure subsystem to Chilean border router & Observatory Operations \\\hline
Network transport on UIUC campus to prompt processing installation area in NPCF & University of Illinois networking, NCSA networking \\\hline
OCS interfaces (bridge, telemetry injection) & Observatory Operations \\\hline
CDS interface & Observatory Operations \\\hline
Base Center computing room resources & Observatory Operations, Computing Facility manager \\\hline
Local assistance in Chile & TBD (if any) \\\hline
ITC for Prompt system, exclusive of reliances listed otherwise & LDF ITC group or relied upon NCSA groups \\\hline
NCSA/NPCF facility resource management & NCSA/NPCF facility management \\\hline
Service-specific code and service-level performance as a part of the overall system, component-level aspect of prompt processing internals & Interfaces provided by LDF software group \\\hline
\end{longtable}

\begin{longtable}{|p{0.3\textwidth}|p{0.3\textwidth}|p{0.4\textwidth}|}
\caption{Uses of Monitoring Service Data Products} \\\hline
\textbf{Entity} & \textbf{Need} & \textbf{Notes} \\\hline
Incident response & Events indicating service faults. & TBD these directly generate notifications (page), (and have the right filtering semantics) \\\hline
Problem management & Incident information and information about marginal or near-miss events detected. & \\\hline
Observing Operations, DPP Staff, and LSST HQ & Comfort displays indicating real-time status of services used. & NCSA staff should be able to see the same information as Observatory Operations staff to prevent confusion in incident response. It is important to note that monitoring relied on by Observatory Operations in Chile needs to be independent of NCSA. Monitoring at each site needs to operate and provide appropriate subsets of information to each site, should the connectivity between sites be disrupted. \\\hline
Alert users & Information about when alerts are being exported and flows to various broker-like entities. & \\\hline
Availability management & Queries, reports and displays focused on historical contributions to failures by reliance. & \\\hline
Capacity management & Queries, reports and displays focused on historical usage of resources. & \\\hline
Contract and SLA management & Queries, reports, and displays of quantities related to performance, e.g., response times, quality of materials or services. & \\\hline
ITC staff & Supplemental information to ITC monitoring. & \\\hline
\end{longtable}

\subparagraph{Example: Wide Area Networks}

Many of the hardware components which make up the WAN will be managed by
different entities (ISPs) based on who owns the particular section of the
network. Typically all ISPs run their own SNMP network to monitor the health of
the devices.

\paragraph{Operational Scenarios}

A predefined hierarchy of roles which will include different levels of users as
listed below:

\begin{itemize}

\item Generic User
\item System Administrator
\item Science User
\item LSST DM Administrator
\item Hardware Operator (ISPs)
\item Camera Control System Administrator
\item Observatory Control System Administrator
\item Super User

\end{itemize}

The level of access and response capabilities will be as defined in the
user-profile. In the case of a ``Generic User,'' it may be necessary only to show
if the LSST system is up and running. There can be a graphical representation of
the status of the systems and subsystems.

For a ``Super User'' who will have access to detailed status information on the
systems and subsystems, will be able to see in-depth event history and status
reports (through log-scraping and fault databases). The Super User will also be
able to access the logs database through the same portal.

In-between levels of access will be defined as per the definition of the roles
and responsibilities of the user.
